import os


import requests
from lxml import etree

class Tieba():


    def __init__(self,  path='',url="http://www.biqugew.com/book/47272/"):
        self.url = url
        self.name=''
        self.path = r'f:\360Downloads'
        self.filename=""
        self.host=r'http://www.biqugew.com'
        # self.url = 'https://tieba.baidu.com/f?ie=utf-8&kw={}'.format(theme)
        self.headers = {
            "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36"}

    def get_url_list(self):

        h="""Accept:text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8
            Connection:keep-alive
            Host:www.biqugew.com
            If-None-Match:1556175321|
            Referer:https://www.baidu.com/link?url=BTqq8AeI13GbOYGb6In1V2dtQbEQG1pytySRjJmrC4bTmdDHnnNv3pjvkD6pWACm&wd=&eqid=c57b34ba000089c4000000055cc13a6a
            Upgrade-Insecure-Requests:1
            User-Agent:Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36"""
        hd=self.hd(h)
        r= requests.get(self.url,headers=hd).content.decode('gbk')
        # print(r)
        html=etree.HTML(r)
        mlist=html.xpath('//div[@id="list"]/dl/dd/a')
        self.name=html.xpath('//div[@id="info"]/h1/text()')[0]
        # print(self.name)
        print(len(mlist))
        url_list=[]
        for i in mlist:
            k=i.text
            v=i.xpath(r"./@href")[0] if len(i.xpath(r"./@href"))>0 else None
            url_list.append((k,v))
        print(url_list)
        return url_list

    def hd(self, headers):
        """处理文件头"""
        a = headers.strip('\n')
        try:
            l = a.split("\n")
            l1 = []
            l2 = []
            head = dict()

            for each in l:
                key = each.split(":")[0].strip()
                val = each.split(":", maxsplit=1)[1].strip()
                head[key] = val
            print(head)
            return head
        except:
            print("输入的header有错误")

    def parse_url(self, url):
        r = requests.get(url,headers=self.headers)
        return r.content.decode('gbk')

    def make_dir(self):
        """如果文件夹不存在则新建"""
        if not self.path:
            return ""
        dir = self.path
        if not os.path.exists(dir):
            os.mkdir(dir)

    def save_file(self, file, content):

        """保存到文件中去"""


        try:
            with open(file, 'a', encoding="utf-8") as f:
                f.write(content)

                print("完成")
        except OSError as r:
            print("打开文件出错了,原因是：", r)

    def run(self):
        """1获得贴吧网址列表"""
        url_list = self.get_url_list()
        # 如果没有则新建一个
        self.make_dir()
        self.filename=self.name+r".txt"
        file_path=os.path.join(self.path,self.filename)
        # 列表循环取内容
        chapter_num=0
        # 显示进度条
        j=len(url_list)
        for url in url_list:
            chapter_num+=1
            # 取回网址解析结果
            print(self.host+url[1])
            content = self.parse_url(self.host+ url[1])
            html = etree.HTML(content)
            mlist = html.xpath(r'//div[@id="content"]/text()')
            c="\n\r"+url[0]+"\n\r"
            for i in mlist:
                c+=i
            # 写入文件
            self.save_file(file_path, c)
            print('共{}章，已经完成了{}章'.format(j,chapter_num))


if __name__ == '__main__':
    # t = Tieba(path=r"f:\360Downloads",url=r'http://www.biqugew.com/book/47272/')
    t = Tieba(path=r"f:\360Downloads",url=r'http://www.biqugew.com/book/112835/')
    t.run()
